{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Parte 1"
      ],
      "metadata": {
        "id": "glATmUy-2WL4"
      },
      "id": "glATmUy-2WL4"
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "import random\n",
        "import numpy as np\n",
        "import math"
      ],
      "metadata": {
        "id": "b6CmtclR4Rk2"
      },
      "id": "b6CmtclR4Rk2",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metodos de validacion"
      ],
      "metadata": {
        "id": "13RQMbA901_I"
      },
      "id": "13RQMbA901_I"
    },
    {
      "cell_type": "code",
      "source": [
        "def hold_out(dataset, test_size=0.2):\n",
        "    dataset = dataset[:]\n",
        "    random.shuffle(dataset)\n",
        "    split_idx = int(len(dataset) * (1 - test_size))\n",
        "    train_set = dataset[:split_idx]\n",
        "    test_set = dataset[split_idx:]\n",
        "    return train_set, test_set\n",
        "\n",
        "def k_fold_split(dataset, k=10):\n",
        "    dataset = dataset[:]\n",
        "    random.shuffle(dataset)\n",
        "    fold_size = len(dataset) // k\n",
        "    folds = [dataset[i * fold_size:(i + 1) * fold_size] for i in range(k)]\n",
        "    return folds"
      ],
      "metadata": {
        "id": "suO5imK501Sq"
      },
      "id": "suO5imK501Sq",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clasificador euclidiano y 1NN"
      ],
      "metadata": {
        "id": "A2_eEZ2H1C57"
      },
      "id": "A2_eEZ2H1C57"
    },
    {
      "cell_type": "code",
      "source": [
        "# Funciones previamente definidas\n",
        "def euclidean_distance(point1, point2):\n",
        "    return math.sqrt(sum((x - y) ** 2 for x, y in zip(point1, point2)))\n",
        "\n",
        "def calculate_centroids(train_set):\n",
        "    class_points = {}\n",
        "    for point, label in train_set:\n",
        "        if label not in class_points:\n",
        "            class_points[label] = []\n",
        "        class_points[label].append(point)\n",
        "    centroids = {}\n",
        "    for label, points in class_points.items():\n",
        "        num_points = len(points)\n",
        "        centroid = [sum(coord) / num_points for coord in zip(*points)]\n",
        "        centroids[label] = centroid\n",
        "    return centroids\n",
        "\n",
        "def euclidean_classifier(train_set, test_point):\n",
        "    centroids = calculate_centroids(train_set)\n",
        "    min_distance = float('inf')\n",
        "    predicted_label = None\n",
        "    for label, centroid in centroids.items():\n",
        "        distance = euclidean_distance(test_point, centroid)\n",
        "        if distance < min_distance:\n",
        "            min_distance = distance\n",
        "            predicted_label = label\n",
        "    return predicted_label\n",
        "\n",
        "def one_nn_classifier(train_set, test_point):\n",
        "    nearest_neighbor = None\n",
        "    min_distance = float('inf')\n",
        "    for train_point, label in train_set:\n",
        "        distance = euclidean_distance(test_point, train_point)\n",
        "        if distance < min_distance:\n",
        "            min_distance = distance\n",
        "            nearest_neighbor = label\n",
        "    return nearest_neighbor"
      ],
      "metadata": {
        "id": "ibgCd9Z51qPs"
      },
      "id": "ibgCd9Z51qPs",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carga del dataset"
      ],
      "metadata": {
        "id": "GTzSNpls4CZo"
      },
      "id": "GTzSNpls4CZo"
    },
    {
      "cell_type": "code",
      "source": [
        "def load_glass_dataset(filepath):\n",
        "    dataset = []\n",
        "    with open(filepath, 'r') as file:\n",
        "        for line in file:\n",
        "            values = line.strip().split(',')\n",
        "            features = list(map(float, values[1:-1]))  # Excluir ID y etiqueta\n",
        "            label = int(values[-1])  # La etiqueta\n",
        "            dataset.append((features, label))\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "vjn_sEZh36h6"
      },
      "id": "vjn_sEZh36h6",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy"
      ],
      "metadata": {
        "id": "vNgKMglG2S_y"
      },
      "id": "vNgKMglG2S_y"
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_score(true_labels, predicted_labels):\n",
        "    correct_predictions = sum(1 for true, pred in zip(true_labels, predicted_labels) if true == pred)\n",
        "    return correct_predictions / len(true_labels)\n"
      ],
      "metadata": {
        "id": "zT1vFvzo2Q2I"
      },
      "id": "zT1vFvzo2Q2I",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejecucion"
      ],
      "metadata": {
        "id": "SHQkITKF6NbD"
      },
      "id": "SHQkITKF6NbD"
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar dataset\n",
        "filepath = \"glass.data\"\n",
        "dataset = load_glass_dataset(filepath)\n",
        "\n",
        "# Hold-Out Evaluation\n",
        "print(\"\\nHold-Out Evaluation:\")\n",
        "# Dividir el dataset en entrenamiento y prueba\n",
        "train_set, test_set = hold_out(dataset, test_size=0.2)\n",
        "\n",
        "# Separar características y etiquetas para SMOTE\n",
        "X_train = [point for point, _ in train_set]\n",
        "y_train = [label for _, label in train_set]\n",
        "\n",
        "# Mostrar distribución antes del sobremuestreo\n",
        "counter = Counter(y_train)\n",
        "print(\"Before SMOTE (Hold-Out):\", counter)\n",
        "\n",
        "# Aplicar SMOTE\n",
        "smt = SMOTE(random_state=42)\n",
        "X_train_sm, y_train_sm = smt.fit_resample(X_train, y_train)\n",
        "\n",
        "# Mostrar distribución después del sobremuestreo\n",
        "counter = Counter(y_train_sm)\n",
        "print(\"After SMOTE (Hold-Out):\", counter)\n",
        "\n",
        "# Reconstruir el conjunto de entrenamiento sobremuestreado\n",
        "train_set_sm = list(zip(X_train_sm, y_train_sm))\n",
        "\n",
        "# Separar características y etiquetas del conjunto de prueba\n",
        "X_test = [point for point, _ in test_set]\n",
        "y_test = [label for _, label in test_set]\n",
        "\n",
        "# Clasificación antes del SMOTE\n",
        "predictions_1nn = [one_nn_classifier(train_set, point) for point in X_test]\n",
        "predictions_euclidean = [euclidean_classifier(train_set, point) for point in X_test]\n",
        "acc_1nn_before = accuracy_score(y_test, predictions_1nn)\n",
        "acc_euclidean_before = accuracy_score(y_test, predictions_euclidean)\n",
        "print(f\"1-NN Accuracy Before SMOTE: {acc_1nn_before * 100:.2f}%\")\n",
        "print(f\"Euclidean Classifier Accuracy Before SMOTE: {acc_euclidean_before * 100:.2f}%\")\n",
        "\n",
        "# Clasificación después del SMOTE\n",
        "predictions_1nn_sm = [one_nn_classifier(train_set_sm, point) for point in X_test]\n",
        "predictions_euclidean_sm = [euclidean_classifier(train_set_sm, point) for point in X_test]\n",
        "acc_1nn_sm = accuracy_score(y_test, predictions_1nn_sm)\n",
        "acc_euclidean_sm = accuracy_score(y_test, predictions_euclidean_sm)\n",
        "print(f\"1-NN Accuracy After SMOTE: {acc_1nn_sm * 100:.2f}%\")\n",
        "print(f\"Euclidean Classifier Accuracy After SMOTE: {acc_euclidean_sm * 100:.2f}%\")\n",
        "\n",
        "# 10-Fold Cross Validation Evaluation\n",
        "print(\"\\n10-Fold Cross Validation Evaluation:\")\n",
        "k = 10\n",
        "folds = k_fold_split(dataset, k)\n",
        "\n",
        "acc_1nn_before = []\n",
        "acc_euclidean_before = []\n",
        "acc_1nn_after = []\n",
        "acc_euclidean_after = []\n",
        "\n",
        "for i in range(k):\n",
        "    # Separar el fold actual para prueba\n",
        "    test_set = folds[i]\n",
        "    train_set = [point for j, fold in enumerate(folds) if j != i for point in fold]\n",
        "\n",
        "    # Dividir características y etiquetas para SMOTE\n",
        "    X_train = [point for point, _ in train_set]\n",
        "    y_train = [label for _, label in train_set]\n",
        "\n",
        "    # Sobremuestreo con SMOTE\n",
        "    smt = SMOTE(random_state=42)\n",
        "    X_train_sm, y_train_sm = smt.fit_resample(X_train, y_train)\n",
        "    train_set_sm = list(zip(X_train_sm, y_train_sm))\n",
        "\n",
        "    # Separar características y etiquetas del conjunto de prueba\n",
        "    X_test = [point for point, _ in test_set]\n",
        "    y_test = [label for _, label in test_set]\n",
        "\n",
        "    # Clasificación antes del SMOTE\n",
        "    predictions_1nn = [one_nn_classifier(train_set, point) for point in X_test]\n",
        "    predictions_euclidean = [euclidean_classifier(train_set, point) for point in X_test]\n",
        "    acc_1nn_before.append(accuracy_score(y_test, predictions_1nn))\n",
        "    acc_euclidean_before.append(accuracy_score(y_test, predictions_euclidean))\n",
        "\n",
        "    # Clasificación después del SMOTE\n",
        "    predictions_1nn_sm = [one_nn_classifier(train_set_sm, point) for point in X_test]\n",
        "    predictions_euclidean_sm = [euclidean_classifier(train_set_sm, point) for point in X_test]\n",
        "    acc_1nn_after.append(accuracy_score(y_test, predictions_1nn_sm))\n",
        "    acc_euclidean_after.append(accuracy_score(y_test, predictions_euclidean_sm))\n",
        "\n",
        "# Promediar los resultados\n",
        "print(f\"1-NN Accuracy Before SMOTE (10-Fold): {sum(acc_1nn_before) / k * 100:.2f}%\")\n",
        "print(f\"Euclidean Classifier Accuracy Before SMOTE (10-Fold): {sum(acc_euclidean_before) / k * 100:.2f}%\")\n",
        "print(f\"1-NN Accuracy After SMOTE (10-Fold): {sum(acc_1nn_after) / k * 100:.2f}%\")\n",
        "print(f\"Euclidean Classifier Accuracy After SMOTE (10-Fold): {sum(acc_euclidean_after) / k * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6D_wsG6V4sQP",
        "outputId": "f5919a91-837c-4184-a694-82100965c9ee"
      },
      "id": "6D_wsG6V4sQP",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Hold-Out Evaluation:\n",
            "Before SMOTE (Hold-Out): Counter({2: 66, 1: 51, 7: 21, 3: 14, 5: 12, 6: 7})\n",
            "After SMOTE (Hold-Out): Counter({7: 66, 5: 66, 2: 66, 1: 66, 3: 66, 6: 66})\n",
            "1-NN Accuracy Before SMOTE: 86.05%\n",
            "Euclidean Classifier Accuracy Before SMOTE: 41.86%\n",
            "1-NN Accuracy After SMOTE: 83.72%\n",
            "Euclidean Classifier Accuracy After SMOTE: 34.88%\n",
            "\n",
            "10-Fold Cross Validation Evaluation:\n",
            "1-NN Accuracy Before SMOTE (10-Fold): 73.81%\n",
            "Euclidean Classifier Accuracy Before SMOTE (10-Fold): 40.48%\n",
            "1-NN Accuracy After SMOTE (10-Fold): 71.43%\n",
            "Euclidean Classifier Accuracy After SMOTE (10-Fold): 40.48%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parte 2"
      ],
      "metadata": {
        "id": "lZwtibNs6gSL"
      },
      "id": "lZwtibNs6gSL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perceptron simple"
      ],
      "metadata": {
        "id": "RYiRJ6i96x53"
      },
      "id": "RYiRJ6i96x53"
    },
    {
      "cell_type": "code",
      "source": [
        "class Perceptron:\n",
        "    def __init__(self, input_size, learning_rate=0.01, epochs=100):\n",
        "        self.input_size = input_size  # Tamaño de la entrada (número de características)\n",
        "        self.learning_rate = learning_rate  # Tasa de aprendizaje\n",
        "        self.epochs = epochs  # Número de épocas (iteraciones)\n",
        "        self.weights = np.zeros(input_size + 1)  # Inicializar pesos (incluyendo el sesgo)\n",
        "\n",
        "    def activation(self, x):\n",
        "        # Función de activación escalón\n",
        "        return 1 if x >= 0 else 0\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        # Calcula la salida del perceptrón\n",
        "        inputs = np.array(inputs)\n",
        "        weighted_sum = np.dot(inputs, self.weights[1:]) + self.weights[0]\n",
        "        return self.activation(weighted_sum)\n",
        "\n",
        "    def train(self, training_inputs, labels):\n",
        "        # Entrenamiento del Perceptrón\n",
        "        for epoch in range(self.epochs):\n",
        "            for inputs, label in zip(training_inputs, labels):\n",
        "                prediction = self.predict(inputs)\n",
        "                error = label - prediction\n",
        "                # Actualizar los pesos\n",
        "                self.weights[1:] += self.learning_rate * error * np.array(inputs)\n",
        "                self.weights[0] += self.learning_rate * error  # Actualización del sesgo\n",
        "\n",
        "    def evaluate(self, test_inputs, test_labels):\n",
        "        # Evaluar el rendimiento del modelo\n",
        "        predictions = [self.predict(inputs) for inputs in test_inputs]\n",
        "        accuracy = np.mean(np.array(predictions) == np.array(test_labels))\n",
        "        return accuracy"
      ],
      "metadata": {
        "id": "uoNkQpyh6i-j"
      },
      "id": "uoNkQpyh6i-j",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cargar dataset"
      ],
      "metadata": {
        "id": "37oitzKh7XaX"
      },
      "id": "37oitzKh7XaX"
    },
    {
      "cell_type": "code",
      "source": [
        "def load_iris_dataset(filepath):\n",
        "    iris_data = []\n",
        "    with open(filepath, 'r') as file:\n",
        "        for line in file:\n",
        "            values = line.strip().split(',')\n",
        "            features = list(map(float, values[:-1]))  # Características (sin la clase)\n",
        "            label = values[-1]\n",
        "\n",
        "            # Filtrar solo Setosa y Virginica\n",
        "            if label == 'Iris-setosa':\n",
        "                label = 0  # Clase 0 para Setosa\n",
        "            elif label == 'Iris-virginica':\n",
        "                label = 1  # Clase 1 para Virginica\n",
        "            else:\n",
        "                continue  # Ignorar otras clases\n",
        "\n",
        "            iris_data.append((features, label))\n",
        "    return iris_data"
      ],
      "metadata": {
        "id": "534lHjQO7Zas"
      },
      "id": "534lHjQO7Zas",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejecucion"
      ],
      "metadata": {
        "id": "XgMLhkit7a3J"
      },
      "id": "XgMLhkit7a3J"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Cargar el dataset Iris (Setosa vs Virginica)\n",
        "filepath = \"iris.data\"\n",
        "dataset = load_iris_dataset(filepath)\n",
        "\n",
        "# Dividir el dataset en entrenamiento y prueba (70/30)\n",
        "train_set, test_set = hold_out(dataset, test_size=0.3)\n",
        "\n",
        "# Separar características y etiquetas para entrenamiento y prueba\n",
        "X_train = [point for point, _ in train_set]\n",
        "y_train = [label for _, label in train_set]\n",
        "X_test = [point for point, _ in test_set]\n",
        "y_test = [label for _, label in test_set]\n",
        "\n",
        "# Crear un perceptrón\n",
        "perceptron = Perceptron(input_size=4, learning_rate=0.1, epochs=100)\n",
        "\n",
        "# Entrenar el perceptrón\n",
        "perceptron.train(X_train, y_train)\n",
        "\n",
        "# Evaluar el perceptrón\n",
        "accuracy = perceptron.evaluate(X_test, y_test)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Probar con nuevas entradas\n",
        "for test in X_test:\n",
        "    print(f\"Input: {test}, Prediction: {perceptron.predict(test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqkRZgS27cek",
        "outputId": "52680cd0-87ef-4bd5-ac5b-ff66de7d7cfe"
      },
      "id": "tqkRZgS27cek",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 100.00%\n",
            "Input: [4.6, 3.2, 1.4, 0.2], Prediction: 0\n",
            "Input: [5.2, 3.5, 1.5, 0.2], Prediction: 0\n",
            "Input: [6.7, 3.1, 5.6, 2.4], Prediction: 1\n",
            "Input: [7.7, 3.8, 6.7, 2.2], Prediction: 1\n",
            "Input: [5.2, 4.1, 1.5, 0.1], Prediction: 0\n",
            "Input: [4.9, 3.1, 1.5, 0.1], Prediction: 0\n",
            "Input: [7.2, 3.2, 6.0, 1.8], Prediction: 1\n",
            "Input: [4.5, 2.3, 1.3, 0.3], Prediction: 0\n",
            "Input: [4.8, 3.1, 1.6, 0.2], Prediction: 0\n",
            "Input: [6.9, 3.1, 5.1, 2.3], Prediction: 1\n",
            "Input: [6.3, 2.5, 5.0, 1.9], Prediction: 1\n",
            "Input: [5.1, 3.8, 1.9, 0.4], Prediction: 0\n",
            "Input: [5.7, 4.4, 1.5, 0.4], Prediction: 0\n",
            "Input: [6.4, 2.8, 5.6, 2.2], Prediction: 1\n",
            "Input: [4.8, 3.0, 1.4, 0.1], Prediction: 0\n",
            "Input: [6.4, 2.7, 5.3, 1.9], Prediction: 1\n",
            "Input: [6.5, 3.0, 5.8, 2.2], Prediction: 1\n",
            "Input: [6.3, 3.3, 6.0, 2.5], Prediction: 1\n",
            "Input: [7.9, 3.8, 6.4, 2.0], Prediction: 1\n",
            "Input: [6.7, 3.3, 5.7, 2.5], Prediction: 1\n",
            "Input: [5.8, 2.8, 5.1, 2.4], Prediction: 1\n",
            "Input: [5.8, 2.7, 5.1, 1.9], Prediction: 1\n",
            "Input: [6.3, 2.9, 5.6, 1.8], Prediction: 1\n",
            "Input: [5.3, 3.7, 1.5, 0.2], Prediction: 0\n",
            "Input: [5.1, 3.7, 1.5, 0.4], Prediction: 0\n",
            "Input: [6.9, 3.1, 5.4, 2.1], Prediction: 1\n",
            "Input: [6.2, 3.4, 5.4, 2.3], Prediction: 1\n",
            "Input: [6.1, 2.6, 5.6, 1.4], Prediction: 1\n",
            "Input: [4.3, 3.0, 1.1, 0.1], Prediction: 0\n",
            "Input: [7.1, 3.0, 5.9, 2.1], Prediction: 1\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}